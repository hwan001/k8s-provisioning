

### k8s-provisioning 파드 배포 시 CPU 부족 문제

k8s-console을 배포하면서 fe, be, agent의 helm chart를 작성하고 내 클러스터에 설치해봤다. 
잘 작동은 했는데 istio를 사용하고 있어서 네임스페이스에 istio-injection 어노테이션을 설정해줬더니 사이드카 주입이 되면서 파드들이 다시 잘 올라갔다.

![pods 상태](images/2025-11-03-01.png)

문제는 agent 배포했더니 pending 이 되었는데, 확인해보니까 아래 같은 에러가 나왔다.

```plain
0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 1 No preemption victims found for incoming pod, 1 Preemption is not helpful for scheduling.
```

스케줄러가 현재 파드를 배치할 수 있는 노드가 없다고 알려주는거 같은데, 아래처럼 확인해보니까 요청된 vCPU의 총량(3910m (97%))이 많아서 예약 가능한 남은 자원이 부족했다.

```bash
kubectl describe node k8sw1 | grep -A10 "Allocated resources"
```
```plain
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                3910m (97%)   37500m (937%)
  memory             7756Mi (32%)  21332Mi (89%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:              <none>
```

`kubectl top nodes`로 확인했을 때는 아래처럼 여유있게 나와서 리소스가 많이 남아 있다고 생각했었는데, 쿠버네티스는 Requests와 Limits을 기준으로 리소스 관리하다보니 지금 당장은 사용하지 않아도 예약되어 있는 자원이 많다는 걸 다시 한번 확인하게 되었다.

```plain
NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8sm1   156m         7%     2642Mi          33%       
k8sw1   250m         6%     8696Mi          36%       
```

데이터 플레인 노드를 추가로 구성해서 클러스터에 추가(join)시키는게 좋아보이긴 하지만 당장 가용한 서버가 없다보니 노드 추가는 시간이 조금 걸릴 것 같다. 그리고 효율적인 리소스 관리 방법에 대한 고민이 좀 필요할 것 같다.

